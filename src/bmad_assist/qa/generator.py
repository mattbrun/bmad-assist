"""QA plan generator.

Story: Standalone QA Plan Workflow
Generates E2E test plans for completed epics using embedded prompt with UX elements.

Architecture:
- Uses embedded prompt (proven to work well)
- Loads UX elements for Category B selector guidance
- Context files: epic, stories, trace, ux-elements
"""

from __future__ import annotations

import logging
import re
from collections.abc import Callable
from dataclasses import dataclass
from pathlib import Path

from bmad_assist.core.config import Config
from bmad_assist.core.types import EpicId
from bmad_assist.providers import get_provider
from bmad_assist.qa.checker import get_qa_plan_path, get_trace_path

logger = logging.getLogger(__name__)


@dataclass
class QAPlanResult:
    """Result of QA plan generation attempt.

    Attributes:
        success: Whether generation succeeded.
        epic_id: Epic that was processed.
        qa_plan_path: Path to generated QA plan (if success).
        trace_path: Path to traceability file (if generated).
        error: Error message if generation failed.
        skipped: True if skipped (already exists).

    """

    success: bool
    epic_id: EpicId
    qa_plan_path: Path | None = None
    trace_path: Path | None = None
    error: str | None = None
    skipped: bool = False

    @classmethod
    def skip(cls, epic_id: EpicId, qa_plan_path: Path) -> QAPlanResult:
        """Create a skipped result (plan already exists)."""
        return cls(
            success=True,
            epic_id=epic_id,
            qa_plan_path=qa_plan_path,
            skipped=True,
        )

    @classmethod
    def fail(cls, epic_id: EpicId, error: str) -> QAPlanResult:
        """Create a failed result."""
        return cls(
            success=False,
            epic_id=epic_id,
            error=error,
        )


def _get_epic_trace(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
) -> Path | None:
    """Get epic-level traceability file if it exists.

    Epic trace is generated by testarch-trace workflow with gate_type=epic.
    This function only checks for existing trace - it does NOT generate one.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.

    Returns:
        Path to trace file if exists, None otherwise.

    """
    trace_path = get_trace_path(config, project_path, epic_id)

    if trace_path.exists():
        logger.debug("Epic trace found: %s", trace_path)
        return trace_path

    logger.debug("No epic trace for %s at %s", epic_id, trace_path)
    return None


def _extract_story_test_context(story_content: str, story_name: str) -> str:
    """Extract only test-relevant sections from a story.

    Pulls out Acceptance Criteria, Technical Notes, and UI-related content
    to keep prompt focused on what matters for E2E tests.

    Args:
        story_content: Full story markdown content.
        story_name: Story filename for header.

    Returns:
        Extracted test-relevant content.

    """
    lines = story_content.split("\n")
    result_lines = [f"### {story_name}"]

    in_section = False

    # Sections we want to extract
    test_sections = {
        "acceptance criteria",
        "technical notes",
        "implementation notes",
        "api",
        "ui",
        "endpoints",
        "routes",
        "selectors",
        "data-testid",
    }

    for line in lines:
        stripped = line.strip().lower()

        # Check for section headers (## or ###)
        if stripped.startswith("#"):
            header_text = stripped.lstrip("#").strip()
            # Check if this is a section we want
            in_section = any(kw in header_text for kw in test_sections)
            if in_section:
                result_lines.append(line)
        elif in_section or any(
            kw in stripped
            for kw in ["data-testid", "endpoint", "/api/", "ui:", "click", "button", "panel"]
        ):
            result_lines.append(line)

    extracted = "\n".join(result_lines).strip()

    # If we got very little, include first 500 chars as fallback
    if len(extracted) < 200:
        extracted = f"### {story_name}\n{story_content[:800]}..."

    return extracted


def _load_requirements(project_path: Path) -> str:
    """Load FR and NFR sections from PRD for test traceability.

    Extracts Functional Requirements and Non-Functional Requirements
    sections from PRD to enable proper test-to-requirement mapping.

    Args:
        project_path: Project root directory.

    Returns:
        FR/NFR content or empty string if PRD not found.

    """
    prd_path = project_path / "docs" / "prd.md"
    if not prd_path.exists():
        logger.debug("PRD not found at %s", prd_path)
        return ""

    try:
        content = prd_path.read_text(encoding="utf-8")

        # Extract FR section
        fr_start = content.find("## Functional Requirements")
        nfr_start = content.find("## Non-Functional Requirements")

        if fr_start == -1:
            logger.debug("No Functional Requirements section in PRD")
            return ""

        # Extract from FR to end of NFR (or end of file)
        if nfr_start > fr_start:
            # Find end of NFR section (next ## or end)
            nfr_end = content.find("\n## ", nfr_start + 1)
            if nfr_end == -1:
                nfr_end = len(content)
            requirements = content[fr_start:nfr_end]
        else:
            # Only FR, no NFR
            fr_end = content.find("\n## ", fr_start + 1)
            if fr_end == -1:
                fr_end = len(content)
            requirements = content[fr_start:fr_end]

        # Truncate if too large
        if len(requirements) > 10000:
            requirements = requirements[:10000] + "\n\n[TRUNCATED]"

        logger.info("Loaded requirements from PRD: %d bytes", len(requirements))
        return requirements

    except Exception as e:
        logger.warning("Failed to load PRD: %s", e)
        return ""


def _load_ux_elements(project_path: Path) -> str:
    """Load UX elements documentation for Category B selectors.

    Searches multiple locations for ux-elements.md files.
    This is CRITICAL for generating correct Playwright selectors.

    Args:
        project_path: Project root directory.

    Returns:
        UX elements content or empty string if not found.

    """
    ux_paths = [
        project_path / "docs" / "modules" / "dashboard" / "ux-elements.md",
        project_path / "docs" / "modules" / "experiments" / "ux-elements.md",
        project_path / "docs" / "ux-elements.md",
    ]

    all_content: list[str] = []

    for ux_path in ux_paths:
        if ux_path.exists():
            try:
                content = ux_path.read_text(encoding="utf-8")
                all_content.append(f"# From: {ux_path.name}\n\n{content}")
                logger.debug("Found UX elements: %s", ux_path)
            except Exception as e:
                logger.warning("Failed to read %s: %s", ux_path, e)

    if not all_content:
        logger.warning(
            "No ux-elements.md found. Category B tests will need manual selector discovery."
        )
        return ""

    combined = "\n\n---\n\n".join(all_content)
    # Limit size to prevent context overflow
    if len(combined) > 15000:
        combined = combined[:15000] + "\n\n[TRUNCATED]"

    logger.info("Loaded UX elements: %d bytes", len(combined))
    return combined


def _build_qa_plan_prompt(
    config: Config,  # noqa: ARG001 - kept for API compatibility
    project_path: Path,
    epic_id: EpicId,
    trace_path: Path | None,
) -> str:
    """Build prompt for QA plan generation.

    Constructs a prompt that instructs the LLM to generate an E2E test plan
    for the given epic, using available context from project files.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.
        trace_path: Path to traceability file.

    Returns:
        Prompt string for LLM.

    """
    # Load epic file if exists
    epic_content = ""
    epic_patterns = [
        project_path / "docs" / "epics" / f"epic-{epic_id}.md",
        project_path / "docs" / "epics" / f"epic-{epic_id}-*.md",
        project_path / "_bmad-output" / "planning-artifacts" / f"*epic*{epic_id}*.md",
    ]
    for pattern in epic_patterns:
        if pattern.exists():
            epic_content = pattern.read_text(encoding="utf-8")
            break
        # Try glob
        matches = list(pattern.parent.glob(pattern.name)) if "*" in str(pattern) else []
        if matches:
            epic_content = matches[0].read_text(encoding="utf-8")
            break

    # Load trace file (if available)
    trace_content = ""
    if trace_path is not None and trace_path.exists():
        trace_content = trace_path.read_text(encoding="utf-8")

    # Load stories for this epic - extract only test-relevant content (AC, UI, API)
    try:
        from bmad_assist.core.paths import get_paths

        stories_dir = get_paths().stories_dir
    except RuntimeError:
        # Fallback for standalone usage without initialized paths
        stories_dir = project_path / "_bmad-output" / "implementation-artifacts"
    stories_content = ""
    if stories_dir.exists():
        story_files = sorted(stories_dir.glob(f"{epic_id}-*.md"))
        for sf in story_files[:20]:  # Up to 20 stories
            full_content = sf.read_text(encoding="utf-8")
            extracted = _extract_story_test_context(full_content, sf.name)
            stories_content += f"\n\n{extracted}"

    # Load UX elements for Category B selectors
    ux_elements = _load_ux_elements(project_path)

    # Load FR/NFR from PRD for requirement traceability
    requirements = _load_requirements(project_path)

    # Build UX elements section for prompt - HIGHEST PRIORITY SECTION
    ux_section = ""
    if ux_elements:
        ux_section = f"""
## UX Elements Documentation (CRITICAL - READ THIS FIRST!)

**THIS IS THE MOST IMPORTANT SECTION FOR CATEGORY B TESTS.**
The following `data-testid` selectors are the ONLY valid selectors for UI tests.

{ux_elements}

### STRICT Category B Rules - MUST FOLLOW

**DO NOT INVENT:**
- NEVER invent data-testid selectors not listed above
- NEVER use `aria-selected`, `aria-checked` or similar ARIA attributes (not present in UI)
- NEVER use class names like `active`, `selected` (UI uses Tailwind, not semantic)
- NEVER guess option values for select/dropdown elements

**UI TECHNOLOGY:**
- Dashboard uses Alpine.js for state management
- State is controlled via `x-show`, `x-data`, `:class` directives
- Active states use Tailwind classes like `bg-muted`, `text-foreground`
- DO NOT check for active state via class - check visibility or content instead

**VALIDATION PATTERNS (use these instead of class checking):**
- Tab is active: `await expect(tabContent).toBeVisible()` (check content is shown)
- Button is active: visual ring `ring-2 ring-primary` (but don't test this, test functionality)
- Option exists: Use `page.locator('option').filter({{ hasText: 'value' }})` first to verify

**WHEN UNCERTAIN:**
- If a selector is not in the ux-elements list → Category C (Human Verification)
- If testing state requires class inspection → use content/visibility checks instead
- If dropdown options are unknown → verify dynamically or use Category C

### forceClick Helper (required for footer buttons)
```typescript
async function forceClick(page: Page, selector: string) {{
  await page.evaluate((sel: string) => {{
    const el = document.querySelector(sel) as HTMLElement;
    if (el) {{ el.scrollIntoView({{ block: 'center' }}); el.click(); }}
  }}, selector);
}}
```
"""
    else:
        ux_section = """
## UX Elements Documentation

No ux-elements.md found. For Category B tests:
- Mark tests as "requires manual selector discovery"
- Or move UI tests to Category C (Human Verification)
"""

    prompt = f"""You are a Test Architect generating an E2E test plan for Epic {epic_id}.

## Task
Generate a comprehensive E2E test plan document in markdown format.
This must be a COMPLETE test plan with ACTUAL test code, not just a summary.

{ux_section}

## Epic Context (FULL - includes wireframes, AC, UI details)
{epic_content if epic_content else f"Epic {epic_id} (no epic file found)"}

## Stories - Test-Relevant Extracts (AC, UI, API)
{stories_content[:40000] if stories_content else f"Stories for epic {epic_id} (files not found)"}

## Requirements from PRD (for Traceability)
{requirements if requirements else "No PRD requirements found. Map tests to story AC instead."}

## Traceability Data
{trace_content[:15000] if trace_content else "No traceability data available."}

## Project Info
- Project root: `$PROJECT_ROOT` (use this variable, NEVER hardcode paths like /home/user/...)
- Virtual env: `$PROJECT_ROOT/.venv/bin/python`
- CLI command: `$PROJECT_ROOT/.venv/bin/bmad-assist`

## Output Format
Generate the test plan with these sections:

1. **Setup** - Bash setup commands (export PROJECT_ROOT, activate venv, etc.)
2. **Test Categories Summary** - Table with count by category (A/B/C)
3. **Master Checklist** - Table: ID | Test Name | Category | Story | Status
4. **Category A Tests** - Each test in SEPARATE subsection with own bash block
5. **Category B Tests** - Each test in SEPARATE subsection with own typescript block
6. **Category C Tests** - Human verification checklists with steps
7. **Traceability Matrix** - Map each test to FR (from PRD), NFR, AC (from stories), or DoD

## CRITICAL: Test Script Format (DO NOT IGNORE!)

**EACH TEST MUST BE IN ITS OWN SUBSECTION with separate code block!**

DO NOT generate one big file with all tests. Each test needs:
- A markdown header: `### E19-A01: Test Name` or `### E19-B01: Test Name`
- Its own fenced code block (```bash or ```typescript)

**WRONG FORMAT (DO NOT DO THIS):**
```typescript
test.describe('All tests', () => {{
  test('E19-B01: ...') {{ ... }}
  test('E19-B02: ...') {{ ... }}
}});
```

**CORRECT FORMAT (DO THIS):**

### E19-B01: Test name
```typescript
test('E19-B01: test name', async ({{ page }}) => {{
  // test code here
}});
```

### E19-B02: Another test
```typescript
test('E19-B02: another test', async ({{ page }}) => {{
  // test code here
}});
```

Each Category B test MUST:
- Be a standalone `test()` function (NOT inside test.describe)
- Include page navigation: `await page.goto('http://localhost:8765');`
- Include `await page.waitForLoadState('domcontentloaded');` (NOT 'networkidle' - SSE breaks it)
- Use forceClick for footer buttons (experiments-button, settings-button)

## Categories
- **A (Full Automation)**: CLI commands, REST API, file validation - 100% automatable
- **B (Playwright)**: UI tests requiring browser - automatable with Playwright
- **C (Human)**: External integrations, credentials, subjective assessment

## CRITICAL Rules for Test Scripts
1. **NEVER hardcode absolute paths** - Use `$PROJECT_ROOT` variable
2. **Always cleanup** - Add cleanup commands at end of each test block
3. **Use temp directories** - `mktemp -d` for isolation, cleanup after
4. **Include setup section** at document start:
```bash
# Setup - run once before tests
export PROJECT_ROOT="$(pwd)"
cd "$PROJECT_ROOT"
source .venv/bin/activate
```
5. **Each test should be idempotent** - can run multiple times
6. **Include expected exit codes** - `echo "Expected exit code: 0"`
7. **Use trap for cleanup** in complex tests:
```bash
cleanup() {{ rm -rf "$TMPDIR"; }}
trap cleanup EXIT
TMPDIR=$(mktemp -d)
```

## Test ID Format
Use `E{epic_id}-A##` for Category A, `E{epic_id}-B##` for B, `E{epic_id}-C##` for C.

## IMPORTANT
- Generate COMPLETE tests with actual code, not just descriptions
- Each Category A test must have a full bash script block
- Each Category B test must have a full Playwright test function
- Each Category C test must have detailed manual steps
- **Map every test to a requirement** (FR-XX, NFR-XX, AC from story, or DoD item)
- Include requirement ID in test comment: `# Requirement: FR-26, AC-3`

Start your response with:
# E2E Test Plan - Epic {epic_id}

End with the marker:
<!-- QA_PLAN_END -->
"""
    return prompt


def _extract_qa_plan(output: str) -> str | None:
    """Extract QA plan content from LLM output.

    Args:
        output: Raw LLM output.

    Returns:
        Extracted QA plan content, or None if not found.

    """
    # Try to find content between markers
    if "<!-- QA_PLAN_END -->" in output:
        # Find start of plan
        start_match = re.search(r"#\s*E2E Test Plan", output)
        if start_match:
            end_idx = output.find("<!-- QA_PLAN_END -->")
            return output[start_match.start() : end_idx].strip()

    # Fallback: find markdown header
    start_match = re.search(r"#\s*E2E Test Plan", output)
    if start_match:
        return output[start_match.start() :].strip()

    # Last resort: return everything after first #
    if "#" in output:
        return output[output.find("#") :].strip()

    return None


def _run_qa_plan_workflow(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
    qa_plan_path: Path,
    trace_path: Path | None,
) -> bool:
    """Execute qa-plan-generate workflow via LLM provider.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier.
        qa_plan_path: Path where QA plan should be written.
        trace_path: Path to traceability file, or None if not available.

    Returns:
        True if workflow succeeded, False otherwise.

    """
    logger.info("Generating QA plan for epic %s via LLM...", epic_id)

    try:
        # Build prompt
        prompt = _build_qa_plan_prompt(config, project_path, epic_id, trace_path)
        logger.debug("QA plan prompt length: %d chars", len(prompt))

        # Get master provider
        provider = get_provider(config.providers.master.provider)

        # Invoke LLM
        logger.info("Invoking LLM for QA plan generation...")
        result = provider.invoke(
            prompt,
            model=config.providers.master.model,
            timeout=config.timeout,
        )

        if result.exit_code != 0:
            logger.error("LLM invocation failed: %s", result.stderr)
            return False

        # Extract QA plan from output
        qa_content = _extract_qa_plan(result.stdout)
        if not qa_content:
            logger.warning("Could not extract QA plan from LLM output, using raw output")
            qa_content = result.stdout

        # Ensure directory exists and write file
        qa_plan_path.parent.mkdir(parents=True, exist_ok=True)
        qa_plan_path.write_text(qa_content, encoding="utf-8")
        logger.info("QA plan generated: %s", qa_plan_path)
        return True

    except Exception as e:
        logger.error("Failed to generate QA plan for epic %s: %s", epic_id, e)
        return False


def generate_qa_plan(
    config: Config,
    project_path: Path,
    epic_id: EpicId,
    *,
    force: bool = False,
) -> QAPlanResult:
    """Generate QA plan for a completed epic.

    This function orchestrates the full QA plan generation:
    1. Check if plan already exists (skip if so, unless force=True)
    2. Check for existing trace file (optional context)
    3. Run qa-plan-generate via embedded prompt
    4. Return result with paths to generated artifacts

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        epic_id: Epic identifier to generate plan for.
        force: If True, regenerate even if plan exists.

    Returns:
        QAPlanResult with success status and artifact paths.

    Example:
        >>> result = generate_qa_plan(config, project_path, 16)
        >>> if result.success:
        ...     print(f"QA plan: {result.qa_plan_path}")

    """
    qa_plan_path = get_qa_plan_path(config, project_path, epic_id)

    # Check if already exists
    if qa_plan_path.exists() and not force:
        logger.info("QA plan already exists for epic %s, skipping", epic_id)
        return QAPlanResult.skip(epic_id, qa_plan_path)

    # Check for existing epic trace (optional - QA plan can work without)
    trace_path = _get_epic_trace(config, project_path, epic_id)
    if trace_path is None:
        logger.info("No epic trace for %s, generating QA plan without trace data", epic_id)

    # Run QA plan workflow
    if not _run_qa_plan_workflow(config, project_path, epic_id, qa_plan_path, trace_path):
        return QAPlanResult.fail(
            epic_id,
            f"Failed to generate QA plan for epic {epic_id}",
        )

    return QAPlanResult(
        success=True,
        epic_id=epic_id,
        qa_plan_path=qa_plan_path,
        trace_path=trace_path,
    )


def generate_missing_qa_plans(
    config: Config,
    project_path: Path,
    missing_epics: list[EpicId],
    *,
    non_interactive: bool = False,
    prompt_fn: Callable[[str, bool], bool] | None = None,
) -> list[QAPlanResult]:
    """Generate QA plans for multiple epics.

    In interactive mode, prompts user for each epic.
    In non-interactive mode, generates all automatically.

    Args:
        config: Configuration instance.
        project_path: Project root directory.
        missing_epics: List of epic IDs without QA plans.
        non_interactive: If True, skip prompts and generate all.
        prompt_fn: Optional function to prompt user (for testing).
            Signature: prompt_fn(message: str, default: bool) -> bool

    Returns:
        List of QAPlanResult for each epic processed.

    """
    results: list[QAPlanResult] = []

    for epic_id in missing_epics:
        if non_interactive:
            # Auto-generate in non-interactive mode
            logger.info("Auto-generating QA plan for epic %s", epic_id)
            result = generate_qa_plan(config, project_path, epic_id)
            results.append(result)
        else:
            # Interactive mode - use prompt_fn or default to typer
            should_generate = True

            if prompt_fn is not None:
                should_generate = prompt_fn(
                    f"Epic {epic_id} has no QA plan. Generate?",
                    True,  # default value
                )
            else:
                # Use typer for interactive prompt
                try:
                    import typer

                    should_generate = typer.confirm(
                        f"Epic {epic_id} has no QA plan. Generate?",
                        default=True,
                    )
                except Exception:
                    # Fallback to auto-generate if typer unavailable
                    should_generate = True

            if should_generate:
                result = generate_qa_plan(config, project_path, epic_id)
                results.append(result)
            else:
                logger.info("Skipped QA plan for epic %s (user declined)", epic_id)

    return results
