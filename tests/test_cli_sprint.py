"""Tests for sprint CLI commands.

Tests the CLI commands added in Story 20.11:
- sprint generate: Generate sprint-status from epic files
- sprint repair: Repair sprint-status from artifact evidence
- sprint validate: Validate sprint-status against artifacts
- sprint sync: Sync sprint-status from state.yaml
"""

import json
import shutil
from pathlib import Path

import pytest
from typer.testing import CliRunner

from bmad_assist.cli import app
from bmad_assist.cli_utils import (
    EXIT_ERROR,
    EXIT_SUCCESS,
)


@pytest.fixture
def cli_runner() -> CliRunner:
    """Create a CLI test runner."""
    return CliRunner()


@pytest.fixture
def project_dir(tmp_path: Path) -> Path:
    """Create a minimal project directory structure for testing."""
    # Create required directories
    (tmp_path / "docs" / "epics").mkdir(parents=True)
    (tmp_path / "_bmad-output" / "implementation-artifacts" / "stories").mkdir(parents=True)
    (tmp_path / "_bmad-output" / "implementation-artifacts" / "code-reviews").mkdir(parents=True)
    (tmp_path / "_bmad-output" / "implementation-artifacts" / "story-validations").mkdir(
        parents=True
    )
    (tmp_path / ".bmad-assist").mkdir(parents=True)

    return tmp_path


@pytest.fixture
def project_with_epic(project_dir: Path) -> Path:
    """Create a project with a sample epic file."""
    epic_content = """---
epic_num: 1
title: Test Epic
status: in-progress
---

# Epic 1: Test Epic

## Stories

### Story 1.1: First Story
Description of first story.

### Story 1.2: Second Story
Description of second story.

### Story 1.3: Third Story
Description of third story.
"""
    epic_file = project_dir / "docs" / "epics" / "epic-1-test.md"
    epic_file.write_text(epic_content)
    return project_dir


@pytest.fixture
def project_with_sprint_status(project_with_epic: Path) -> Path:
    """Create a project with existing sprint-status.yaml."""
    sprint_content = """# Generated by bmad-assist
generated: 2026-01-01T00:00:00
project: test-project

development_status:
  epic-1: backlog
  1-1-first-story: backlog
  1-2-second-story: in-progress
  1-3-third-story: done
"""
    sprint_path = (
        project_with_epic / "_bmad-output" / "implementation-artifacts" / "sprint-status.yaml"
    )
    sprint_path.write_text(sprint_content)
    return project_with_epic


@pytest.fixture
def project_with_artifacts(project_with_sprint_status: Path) -> Path:
    """Create a project with story artifacts for evidence-based inference."""
    # Create story file for 1-1
    story_1_1 = (
        project_with_sprint_status
        / "_bmad-output"
        / "implementation-artifacts"
        / "stories"
        / "1-1-first-story.md"
    )
    story_1_1.write_text("""---
title: First Story
---

# Story 1.1: First Story

Status: done

Description of first story.
""")

    # Create code review for 1-2 (synthesis = master review)
    review_1_2 = (
        project_with_sprint_status
        / "_bmad-output"
        / "implementation-artifacts"
        / "code-reviews"
        / "synthesis-1-2.md"
    )
    review_1_2.write_text("""# Code Review Synthesis for 1-2

Story is ready.
""")

    # Create story file for 1-3 but no code review
    story_1_3 = (
        project_with_sprint_status
        / "_bmad-output"
        / "implementation-artifacts"
        / "stories"
        / "1-3-third-story.md"
    )
    story_1_3.write_text("""---
title: Third Story
---

# Story 1.3: Third Story

Status: in-progress

Description of third story.
""")

    return project_with_sprint_status


@pytest.fixture
def project_with_state(project_with_sprint_status: Path) -> Path:
    """Create a project with state.yaml for sync testing."""
    state_content = """version: 1
current_epic: 1
current_story: '1.2'
current_phase: dev_story
completed_stories:
  - '1.1'
completed_epics: []
created_at: '2026-01-01T00:00:00+00:00'
updated_at: '2026-01-01T00:00:00+00:00'
"""
    state_path = project_with_sprint_status / ".bmad-assist" / "state.yaml"
    state_path.write_text(state_content)
    return project_with_sprint_status


# =============================================================================
# Test: sprint generate command
# =============================================================================


class TestSprintGenerate:
    """Tests for `bmad-assist sprint generate` command."""

    def test_generate_creates_new_file(
        self, cli_runner: CliRunner, project_with_epic: Path
    ) -> None:
        """Test generate creates new sprint-status when file doesn't exist."""
        result = cli_runner.invoke(app, ["sprint", "generate", "--project", str(project_with_epic)])

        # Should succeed
        assert result.exit_code == EXIT_SUCCESS, result.output

        # Sprint-status should be created
        sprint_path = (
            project_with_epic / "_bmad-output" / "implementation-artifacts" / "sprint-status.yaml"
        )
        assert sprint_path.exists()

        # Check content
        content = sprint_path.read_text()
        assert "1-1-first-story" in content
        assert "1-2-second-story" in content
        assert "epic-1" in content

    def test_generate_merges_with_existing(
        self, cli_runner: CliRunner, project_with_sprint_status: Path
    ) -> None:
        """Test generate merges with existing sprint-status."""
        result = cli_runner.invoke(
            app, ["sprint", "generate", "--project", str(project_with_sprint_status)]
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        assert "Generated" in result.output

    def test_generate_verbose_shows_changes(
        self, cli_runner: CliRunner, project_with_epic: Path
    ) -> None:
        """Test generate --verbose shows detailed changes."""
        result = cli_runner.invoke(
            app, ["sprint", "generate", "--project", str(project_with_epic), "--verbose"]
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        # Should show table of changes when verbose
        # Note: if no changes, verbose just shows summary

    def test_generate_handles_no_epics(self, cli_runner: CliRunner, project_dir: Path) -> None:
        """Test generate handles project with no epic files gracefully."""
        result = cli_runner.invoke(app, ["sprint", "generate", "--project", str(project_dir)])

        # Should still succeed, just with no entries
        assert result.exit_code == EXIT_SUCCESS, result.output


# =============================================================================
# Test: sprint repair command
# =============================================================================


class TestSprintRepair:
    """Tests for `bmad-assist sprint repair` command."""

    def test_repair_applies_evidence(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test repair applies evidence-based inference."""
        result = cli_runner.invoke(
            app, ["sprint", "repair", "--project", str(project_with_artifacts)],
            input="y\n",  # Confirm the overwrite prompt
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        assert "Repaired" in result.output

    def test_repair_dry_run_shows_changes(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test repair --dry-run shows changes without applying."""
        # Get original content
        sprint_path = (
            project_with_artifacts
            / "_bmad-output"
            / "implementation-artifacts"
            / "sprint-status.yaml"
        )
        original_content = sprint_path.read_text()

        result = cli_runner.invoke(
            app, ["sprint", "repair", "--project", str(project_with_artifacts), "--dry-run"]
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        assert "Dry run" in result.output

        # Content should be unchanged
        assert sprint_path.read_text() == original_content

    def test_repair_verbose_shows_details(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test repair --verbose shows divergence details."""
        result = cli_runner.invoke(
            app, ["sprint", "repair", "--project", str(project_with_artifacts), "--verbose"],
            input="y\n",  # Confirm the overwrite prompt
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        # Verbose shows divergence percentage
        assert "Divergence" in result.output or "Repaired" in result.output

    def test_repair_dry_run_verbose(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test repair --dry-run --verbose shows change table."""
        result = cli_runner.invoke(
            app,
            [
                "sprint",
                "repair",
                "--project",
                str(project_with_artifacts),
                "--dry-run",
                "--verbose",
            ],
        )

        assert result.exit_code == EXIT_SUCCESS, result.output
        assert "Dry run" in result.output


# =============================================================================
# Test: sprint validate command
# =============================================================================


class TestSprintValidate:
    """Tests for `bmad-assist sprint validate` command."""

    def test_validate_no_discrepancies(
        self, cli_runner: CliRunner, project_with_sprint_status: Path
    ) -> None:
        """Test validate returns 0 when no ERROR discrepancies."""
        # First repair to ensure consistency
        cli_runner.invoke(app, ["sprint", "repair", "--project", str(project_with_sprint_status)])

        result = cli_runner.invoke(
            app, ["sprint", "validate", "--project", str(project_with_sprint_status)]
        )

        # Should succeed if only WARN or no discrepancies
        # Note: exact exit code depends on state of artifacts
        assert result.exit_code in (EXIT_SUCCESS, EXIT_ERROR)

    def test_validate_reports_errors(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test validate reports ERROR discrepancies."""
        # Manually create a discrepancy: sprint says 'done' but no master review
        sprint_path = (
            project_with_artifacts
            / "_bmad-output"
            / "implementation-artifacts"
            / "sprint-status.yaml"
        )
        content = sprint_path.read_text()
        # 1-1 has Status: done in story file but no master review
        # This shouldn't be an error since explicit Status wins

        result = cli_runner.invoke(
            app, ["sprint", "validate", "--project", str(project_with_artifacts)]
        )

        # Check output contains discrepancy info or success
        assert result.exit_code in (EXIT_SUCCESS, EXIT_ERROR)

    def test_validate_json_output(
        self, cli_runner: CliRunner, project_with_sprint_status: Path
    ) -> None:
        """Test validate --format=json produces valid JSON."""
        result = cli_runner.invoke(
            app,
            [
                "sprint",
                "validate",
                "--project",
                str(project_with_sprint_status),
                "--format",
                "json",
            ],
        )

        # Parse JSON output - extract JSON object from output
        # (may contain log messages before the JSON when run with other tests)
        output = result.stdout
        json_start = output.find("{")
        if json_start == -1:
            pytest.fail(f"No JSON object found in output: {output}")

        try:
            data = json.loads(output[json_start:])
            assert "success" in data
            assert "exit_code" in data
            assert "summary" in data
            assert "discrepancies" in data
        except json.JSONDecodeError:
            pytest.fail(f"Invalid JSON output: {output[json_start:]}")

    def test_validate_missing_sprint_status(self, cli_runner: CliRunner, project_dir: Path) -> None:
        """Test validate fails gracefully when sprint-status missing."""
        result = cli_runner.invoke(app, ["sprint", "validate", "--project", str(project_dir)])

        assert result.exit_code == EXIT_ERROR
        assert "not found" in result.output.lower() or "Sprint-status" in result.output

    def test_validate_verbose_shows_evidence(
        self, cli_runner: CliRunner, project_with_artifacts: Path
    ) -> None:
        """Test validate --verbose shows evidence details."""
        result = cli_runner.invoke(
            app, ["sprint", "validate", "--project", str(project_with_artifacts), "--verbose"]
        )

        # Should include either success message or evidence details
        assert result.exit_code in (EXIT_SUCCESS, EXIT_ERROR)

    def test_validate_invalid_format(
        self, cli_runner: CliRunner, project_with_sprint_status: Path
    ) -> None:
        """Test validate with invalid format fails."""
        result = cli_runner.invoke(
            app,
            ["sprint", "validate", "--project", str(project_with_sprint_status), "--format", "xml"],
        )

        assert result.exit_code == EXIT_ERROR
        assert "Invalid --format" in result.output


# =============================================================================
# Test: sprint sync command
# =============================================================================


class TestSprintSync:
    """Tests for `bmad-assist sprint sync` command."""

    def test_sync_updates_sprint_status(
        self, cli_runner: CliRunner, project_with_state: Path
    ) -> None:
        """Test sync updates sprint-status from state.yaml."""
        result = cli_runner.invoke(app, ["sprint", "sync", "--project", str(project_with_state)])

        assert result.exit_code == EXIT_SUCCESS, result.output
        assert "Synced" in result.output

    def test_sync_missing_state(
        self, cli_runner: CliRunner, project_with_sprint_status: Path
    ) -> None:
        """Test sync fails gracefully when state.yaml missing."""
        result = cli_runner.invoke(
            app, ["sprint", "sync", "--project", str(project_with_sprint_status)]
        )

        assert result.exit_code == EXIT_ERROR
        assert "state.yaml not found" in result.output

    def test_sync_verbose_shows_skipped(
        self, cli_runner: CliRunner, project_with_state: Path
    ) -> None:
        """Test sync --verbose shows skipped keys."""
        result = cli_runner.invoke(
            app, ["sprint", "sync", "--project", str(project_with_state), "--verbose"]
        )

        assert result.exit_code == EXIT_SUCCESS, result.output


# =============================================================================
# Test: Common options
# =============================================================================


class TestSprintCommonOptions:
    """Tests for common options across sprint commands."""

    def test_project_option(self, cli_runner: CliRunner, project_with_epic: Path) -> None:
        """Test --project option works for all commands."""
        # Test with explicit project path
        result = cli_runner.invoke(app, ["sprint", "generate", "-p", str(project_with_epic)])
        assert result.exit_code == EXIT_SUCCESS, result.output

    def test_invalid_project_path(self, cli_runner: CliRunner) -> None:
        """Test commands fail gracefully with invalid project path."""
        result = cli_runner.invoke(app, ["sprint", "generate", "--project", "/nonexistent/path"])
        assert result.exit_code == EXIT_ERROR
        assert "not found" in result.output.lower()

    def test_verbose_flag_short(self, cli_runner: CliRunner, project_with_epic: Path) -> None:
        """Test -v short form for verbose flag."""
        result = cli_runner.invoke(app, ["sprint", "generate", "-p", str(project_with_epic), "-v"])
        assert result.exit_code == EXIT_SUCCESS, result.output


# =============================================================================
# Test: Help output
# =============================================================================


class TestSprintHelp:
    """Tests for sprint command help output."""

    def test_sprint_help(self, cli_runner: CliRunner) -> None:
        """Test sprint --help shows all subcommands."""
        result = cli_runner.invoke(app, ["sprint", "--help"])

        assert result.exit_code == 0
        assert "generate" in result.output
        assert "repair" in result.output
        assert "validate" in result.output
        assert "sync" in result.output

    def test_generate_help(self, cli_runner: CliRunner) -> None:
        """Test sprint generate --help shows options."""
        result = cli_runner.invoke(app, ["sprint", "generate", "--help"])

        assert result.exit_code == 0
        assert "--project" in result.output
        assert "--verbose" in result.output

    def test_repair_help(self, cli_runner: CliRunner) -> None:
        """Test sprint repair --help shows options."""
        result = cli_runner.invoke(app, ["sprint", "repair", "--help"])

        assert result.exit_code == 0
        assert "--project" in result.output
        assert "--verbose" in result.output
        assert "--dry-run" in result.output

    def test_validate_help(self, cli_runner: CliRunner) -> None:
        """Test sprint validate --help shows options."""
        result = cli_runner.invoke(app, ["sprint", "validate", "--help"])

        assert result.exit_code == 0
        assert "--project" in result.output
        assert "--verbose" in result.output
        assert "--format" in result.output

    def test_sync_help(self, cli_runner: CliRunner) -> None:
        """Test sprint sync --help shows options."""
        result = cli_runner.invoke(app, ["sprint", "sync", "--help"])

        assert result.exit_code == 0
        assert "--project" in result.output
        assert "--verbose" in result.output
